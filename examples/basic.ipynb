{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic properties of NestedTensor\n",
    "\n",
    "This notebook illustries some of the basic properties of NestedTensor such as dim, size and nested_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nestedtensor import torch\n",
    "def print_eval(s):\n",
    "    print((\"\\033[1;31m$ \" + s + \":\\033[0m\").ljust(30) + \"\\n{}\\n\".format(str(eval(s))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine the following is a collection of Grey-scale images. The NestedTensor represents a list with two entries. The first entry of that list is a list of two images, the second entry of that list is a list with one image. Maybe these images represent faces and are grouped by the corresponding individual.\n",
    "\n",
    "If you find this example to be too unrealistic, imagine that these Tensors represents sentences group in paragraph and are of int64 instead of float. Or imagine that they represent multi-channel audio files grouped by speaker identitiy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m$ nt:\u001b[0m              \n",
      "nested_tensor([\n",
      "\t[\n",
      "\t\ttensor([[0.7173, 0.5520, 0.9007],\n",
      "\t\t        [0.4070, 0.6530, 0.8580]]),\n",
      "\t\ttensor([[0.6783, 0.9212, 0.8009, 0.4163, 0.9832],\n",
      "\t\t        [0.3170, 0.8630, 0.1032, 0.8065, 0.2567],\n",
      "\t\t        [0.0512, 0.0900, 0.8160, 0.7853, 0.1891],\n",
      "\t\t        [0.5247, 0.8860, 0.6753, 0.0171, 0.6665]])\n",
      "\t],\n",
      "\t[\n",
      "\t\ttensor([[0.9114, 0.7392]])\n",
      "\t]\n",
      "])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nt = torch.nested_tensor(\n",
    "    [\n",
    "        [\n",
    "            torch.rand(2, 3),\n",
    "            torch.rand(4, 5)\n",
    "        ],\n",
    "        [\n",
    "            torch.rand(1, 2)\n",
    "        ]\n",
    "    ])\n",
    "print_eval(\"nt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m$ nt.nested_dim():\u001b[0m \n",
      "2\n",
      "\n",
      "\u001b[1;31m$ nt.tensor_dim():\u001b[0m \n",
      "2\n",
      "\n",
      "\u001b[1;31m$ nt.dim():\u001b[0m        \n",
      "4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Every non-empty NestedTensor is of at least dimension one, because it must represent at least a list.\n",
    "# For each level lists with list entries added we increase the nested dimension by one. That means\n",
    "# this NestedTensor is of dimension two.\n",
    "print_eval(\"nt.nested_dim()\")\n",
    "\n",
    "# The tensor dimension is two, because the Tensor constiuents are of dimension two.\n",
    "print_eval(\"nt.tensor_dim()\")\n",
    "\n",
    "# The dimension is four, because it is the sum of the nested and tensor dimension.\n",
    "print_eval(\"nt.dim()\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m$ nt.dtype:\u001b[0m        \n",
      "torch.float32\n",
      "\n",
      "\u001b[1;31m$ nt.layout:\u001b[0m       \n",
      "torch.strided\n",
      "\n",
      "\u001b[1;31m$ nt.device:\u001b[0m       \n",
      "cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The data type, layout and device of a NestedTensor as unsurprisingly that of the Tensor constiuent.\n",
    "# Just as with torch.tensor these properties must align during construction.\n",
    "print_eval(\"nt.dtype\")\n",
    "print_eval(\"nt.layout\")\n",
    "print_eval(\"nt.device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nested_tensor_from_tensor_mask, torch.NestedTensor.to_tensor_mask and more\n",
    "To put NestedTensors in context of current approaches of dealing with variably sized datapoints, such as padding and masking, we will introduce construction and conversion to tensors with masks and tensors with speical non-data identifying values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m$ tensor:\u001b[0m          \n",
      "tensor([[[0.8413, 0.7325, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.6334, 0.5473, 0.3273, 0.0564],\n",
      "         [0.3023, 0.6826, 0.3519, 0.1804],\n",
      "         [0.8431, 0.1645, 0.1821, 0.9185]]])\n",
      "\n",
      "\u001b[1;31m$ mask:\u001b[0m            \n",
      "tensor([[[ True,  True, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True]]])\n",
      "\n",
      "\u001b[1;31m$ torch.nested_tensor_from_tensor_mask(tensor, mask):\u001b[0m\n",
      "nested_tensor([\n",
      "\ttensor([[0.8413, 0.7325]]),\n",
      "\ttensor([[0.6334, 0.5473, 0.3273, 0.0564],\n",
      "\t        [0.3023, 0.6826, 0.3519, 0.1804],\n",
      "\t        [0.8431, 0.1645, 0.1821, 0.9185]])\n",
      "])\n",
      "\n",
      "\u001b[1;31m$ torch.nested_tensor_from_padded_tensor(tensor, padding=0):\u001b[0m\n",
      "nested_tensor([\n",
      "\ttensor([[0.8413, 0.7325]]),\n",
      "\ttensor([[0.6334, 0.5473, 0.3273, 0.0564],\n",
      "\t        [0.3023, 0.6826, 0.3519, 0.1804],\n",
      "\t        [0.8431, 0.1645, 0.1821, 0.9185]])\n",
      "])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor(\n",
    "        [[[0.8413, 0.7325, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
    "\n",
    "        [[0.6334, 0.5473, 0.3273, 0.0564],\n",
    "         [0.3023, 0.6826, 0.3519, 0.1804],\n",
    "         [0.8431, 0.1645, 0.1821, 0.9185]]])\n",
    "mask = torch.tensor(\n",
    "        [[[ True,  True, False, False],\n",
    "         [False, False, False, False],\n",
    "         [False, False, False, False]],\n",
    "\n",
    "        [[ True,  True,  True,  True],\n",
    "         [ True,  True,  True,  True],\n",
    "         [ True,  True,  True,  True]]])\n",
    "print_eval(\"tensor\")\n",
    "print_eval(\"mask\")\n",
    "nt2 = torch.nested_tensor_from_tensor_mask(tensor, mask)\n",
    "print_eval(\"torch.nested_tensor_from_tensor_mask(tensor, mask)\")\n",
    "print_eval(\"torch.nested_tensor_from_padded_tensor(tensor, padding=0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m$ nt2.to_tensor_mask():\u001b[0m\n",
      "(tensor([[[0.8413, 0.7325, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.6334, 0.5473, 0.3273, 0.0564],\n",
      "         [0.3023, 0.6826, 0.3519, 0.1804],\n",
      "         [0.8431, 0.1645, 0.1821, 0.9185]]]), tensor([[[ True,  True, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True]]]))\n",
      "\n",
      "\u001b[1;31m$ nt2.to_padded_tensor(padding=-10):\u001b[0m\n",
      "tensor([[[  0.8413,   0.7325, -10.0000, -10.0000],\n",
      "         [-10.0000, -10.0000, -10.0000, -10.0000],\n",
      "         [-10.0000, -10.0000, -10.0000, -10.0000]],\n",
      "\n",
      "        [[  0.6334,   0.5473,   0.3273,   0.0564],\n",
      "         [  0.3023,   0.6826,   0.3519,   0.1804],\n",
      "         [  0.8431,   0.1645,   0.1821,   0.9185]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_eval(\"nt2.to_tensor_mask()\")\n",
    "print_eval(\"nt2.to_padded_tensor(padding=-10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nested_size, size and len()** should be part of the bread and butter of a NestedTensor user.\n",
    "\n",
    "Therefore it is important to understand these concepts well.\n",
    "\n",
    "NestedTensor.nested_size is defined as the result of recusrively mapping ```lambda x: x.size()``` onto a NestedTensor's tensor constiuents. Or more loosely defined, it is the result of replacing the Tensor constiuents by their size.\n",
    "\n",
    "NestedTensor.nested_size optionally also accepts a dim argument. This will return a slice across the given dimension. This might be easiest explain via below example.\n",
    "\n",
    "nt.nested_size(0) returns the length of nt or the number of entries in the list it represents. This is very similar to ```list.__len__```.\n",
    "\n",
    "nt.nested_size(1) returns the length of the entries of the outer list.\n",
    "\n",
    "nt.nested_size(2) returns the first entry of each Tensor constiuent's size. \n",
    "\n",
    "nt.nested_size(3) returns the second entry of each Tensor constiuent's size.\n",
    "\n",
    "We will soon define .size and unbind which will make the definition of this even clearer. We will also show some examples that justify these methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m$ nt:\u001b[0m              \n",
      "nested_tensor([\n",
      "\t[\n",
      "\t\ttensor([[0.7173, 0.5520, 0.9007],\n",
      "\t\t        [0.4070, 0.6530, 0.8580]]),\n",
      "\t\ttensor([[0.6783, 0.9212, 0.8009, 0.4163, 0.9832],\n",
      "\t\t        [0.3170, 0.8630, 0.1032, 0.8065, 0.2567],\n",
      "\t\t        [0.0512, 0.0900, 0.8160, 0.7853, 0.1891],\n",
      "\t\t        [0.5247, 0.8860, 0.6753, 0.0171, 0.6665]])\n",
      "\t],\n",
      "\t[\n",
      "\t\ttensor([[0.9114, 0.7392]])\n",
      "\t]\n",
      "])\n",
      "\n",
      "\u001b[1;31m$ nt.nested_size():\u001b[0m\n",
      "torch.NestedSize((\n",
      "\t(\n",
      "\t\ttorch.Size([2, 3]),\n",
      "\t\ttorch.Size([4, 5])\n",
      "\t),\n",
      "\t(\n",
      "\t\ttorch.Size([1, 2])\n",
      "\t)\n",
      "))\n",
      "\n",
      "\u001b[1;31m$ len(nt):\u001b[0m         \n",
      "2\n",
      "\n",
      "\u001b[1;31m$ nt.nested_size(0):\u001b[0m\n",
      "2\n",
      "\n",
      "\u001b[1;31m$ nt.nested_size(1):\u001b[0m\n",
      "(2, 1)\n",
      "\n",
      "\u001b[1;31m$ nt.nested_size(2):\u001b[0m\n",
      "((2, 4), (1,))\n",
      "\n",
      "\u001b[1;31m$ nt.nested_size(3):\u001b[0m\n",
      "((3, 5), (2,))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_eval(\"nt\")\n",
    "print_eval(\"nt.nested_size()\")\n",
    "print_eval(\"len(nt)\")\n",
    "print_eval(\"nt.nested_size(0)\")\n",
    "print_eval(\"nt.nested_size(1)\")\n",
    "print_eval(\"nt.nested_size(2)\")\n",
    "print_eval(\"nt.nested_size(3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NestedTensor.size** is a function that returns a tuple of the format\n",
    "(n_1, n_2, ..., n_nested_dim, t_1, t_2, ..., t_tensor_dim). The sizes lead by n_ are defined \n",
    "to be the nested sizes each at a nested dimension, the sizes lead by t_ are defined to be the \n",
    "tensor sizes each at a tensor dimension. They are a reduced version of nested_size and \n",
    "aim to represent the size across a slice of nested_size.\n",
    "\n",
    "size(i) is of value k if all numerical entries of nested_size(dim) are of value k, otherwise it is None.\n",
    "size() is a tuple with entries size(i)\n",
    "In this case most size(i) will be None, except for the first. We will later see examples of NestedTensors where this is not the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m$ nt.size():\u001b[0m       \n",
      "(2, None, None, None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_eval(\"nt.size()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**unbind** is a fundamental building block of NestedTensors. Applying unbind to a NestedTensor will return the constiuents of the list it represents. More importantly, it returns a few of these elements. It does not take a dim argument, for now, in comparison to torch.Tensor.unbind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nested_tensor([\n",
      "\ttensor([[0.7173, 0.5520, 0.9007],\n",
      "\t        [0.4070, 0.6530, 0.8580]]),\n",
      "\ttensor([[0.6783, 0.9212, 0.8009, 0.4163, 0.9832],\n",
      "\t        [0.3170, 0.8630, 0.1032, 0.8065, 0.2567],\n",
      "\t        [0.0512, 0.0900, 0.8160, 0.7853, 0.1891],\n",
      "\t        [0.5247, 0.8860, 0.6753, 0.0171, 0.6665]])\n",
      "])\n",
      "\n",
      "nested_tensor([\n",
      "\ttensor([[0.9114, 0.7392]])\n",
      "])\n"
     ]
    }
   ],
   "source": [
    "entries = nt.unbind()\n",
    "print(entries[0])\n",
    "print(\"\")\n",
    "print(entries[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nested_tensor([\n",
      "\t[\n",
      "\t\ttensor([[0.7536, 0.8515, 0.6210],\n",
      "\t\t        [0.9183, 0.7943, 0.6540]]),\n",
      "\t\ttensor([[0.6783, 0.9212, 0.8009, 0.4163, 0.9832],\n",
      "\t\t        [0.3170, 0.8630, 0.1032, 0.8065, 0.2567],\n",
      "\t\t        [0.0512, 0.0900, 0.8160, 0.7853, 0.1891],\n",
      "\t\t        [0.5247, 0.8860, 0.6753, 0.0171, 0.6665]])\n",
      "\t],\n",
      "\t[\n",
      "\t\ttensor([[0.9114, 0.7392]])\n",
      "\t]\n",
      "])\n"
     ]
    }
   ],
   "source": [
    "# Edit the first entry of the first list in-place. You can see that the memory is shared between these constructs.\n",
    "entries[0].unbind()[0].cos_()\n",
    "print(nt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
