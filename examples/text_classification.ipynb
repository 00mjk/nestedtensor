{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import io\n",
    "import tarfile\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import sys\n",
    "import concurrent.futures\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import namedtuple\n",
    "\n",
    "from nestedtensor import torch\n",
    "\n",
    "URL = \"https://github.com/le-scientifique/torchDatasets/raw/master/dbpedia_csv.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Point = namedtuple('Point', 'label text')\n",
    "\n",
    "def get_data(URL):\n",
    "    r = requests.get(URL)\n",
    "    file_like_object = io.BytesIO(r.content)\n",
    "    tar = tarfile.open(fileobj=file_like_object)\n",
    "    d = {}\n",
    "    for member in tar.getmembers():\n",
    "        if member.isfile() and member.name.endswith('csv'):\n",
    "            k = 'train' if 'train' in member.name else 'test'\n",
    "            d[k] = tar.extractfile(member)\n",
    "    return d\n",
    "\n",
    "\n",
    "def preprocess(iterator):\n",
    "    def _preprocess(line):\n",
    "        line = line.decode('UTF-8')\n",
    "        line = line.lower()\n",
    "        line = re.sub(r'[^0-9a-zA-Z,\\s]', \"\", line)\n",
    "        line = line.split(',')\n",
    "        label = int(line[0]) - 1\n",
    "        text = (\" \".join(line[1:])).split()\n",
    "        if len(line) > 2:\n",
    "            return Point(label=label, text=text)\n",
    "    for line in iterator:\n",
    "        yield _preprocess(line)\n",
    "\n",
    "\n",
    "def build_vocab(iterator):\n",
    "    counter = Counter()\n",
    "    labels = set()\n",
    "    for point in iterator:\n",
    "        counter.update(point.text)\n",
    "        labels.add(point.label)\n",
    "    vocab = {}\n",
    "    for i, (word, count) in enumerate(counter.most_common()):\n",
    "        vocab[word] = i\n",
    "\n",
    "    return vocab, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(URL)\n",
    "data = {k: list(preprocess(v)) for (k, v) in data.items()}\n",
    "vocab, labels = build_vocab(data['train'])\n",
    "UNK = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        return self.fc(self.embedding(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 10\n",
    "model = TextSentiment(len(vocab) + 1, embed_dim, len(labels)).cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(data):\n",
    "    data = torch.nested_tensor(\n",
    "        [torch.tensor(list(map(lambda x: vocab.get(x, UNK), tokens))) for tokens in data])\n",
    "    return data\n",
    "\n",
    "def yield_data_futures(data):\n",
    "    random.shuffle(data)\n",
    "    labels = []\n",
    "    batch_data = []\n",
    "    futures = []\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=8) as executor:\n",
    "        for i, point in enumerate(data):\n",
    "            # Stop accumulating lines of text once we reach 4000 tokens or more\n",
    "            # This yields variable batch sizes, but with consistent memory pressure\n",
    "            if sum(map(len, batch_data), 0) < 10000:\n",
    "                labels.append(point.label)\n",
    "                batch_data.append(point.text)\n",
    "            else:\n",
    "                if len(futures) < 40:\n",
    "                    futures.append((torch.tensor(labels), executor.submit(create_batch, batch_data)))\n",
    "                else:\n",
    "                    yield futures[0]\n",
    "                    futures = futures[1:]\n",
    "                labels = []\n",
    "                batch_data = []\n",
    "\n",
    "    for future in futures:\n",
    "        yield future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 27205880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time:  21s epoch:   0 lr: 1.000000 loss: 0.767902\n",
      "time:  21s epoch:   1 lr: 0.810000 loss: 0.425534\n",
      "time:  21s epoch:   2 lr: 0.729000 loss: 0.274846\n",
      "time:  21s epoch:   3 lr: 0.656100 loss: 0.274525\n",
      "time:  21s epoch:   4 lr: 0.590490 loss: 0.209720\n"
     ]
    }
   ],
   "source": [
    "num_tokens = sum(map(lambda x: len(x.text), data['train']))\n",
    "print(\"Total number of tokens: {}\".format(num_tokens))\n",
    "for epoch in range(5):\n",
    "    i = 0\n",
    "    t0 = time.time()\n",
    "    for labels, future in yield_data_futures(data['train']):\n",
    "        batch = future.result()\n",
    "        labels = labels.to('cuda', non_blocking=True)\n",
    "        batch = batch.to('cuda', non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 16 == 1:\n",
    "            sys.stderr.write(\n",
    "                \"\\rtime: {:3.0f}s epoch: {:3.0f} lr: {:3.6f} loss: {:3.6f}\".format(\n",
    "                    time.time() - t0, \n",
    "                    epoch, \n",
    "                    scheduler.get_lr()[0],\n",
    "                    loss, \n",
    "                )\n",
    "            )\n",
    "            sys.stderr.flush()\n",
    "        i += batch.numel()\n",
    "    scheduler.step()\n",
    "    sys.stderr.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.940569281578064\n"
     ]
    }
   ],
   "source": [
    "output = [(tb[0], model(tb[1].result().to('cuda')).argmax(1).cpu()) for tb in yield_data_futures(data['test'])]\n",
    "predictions = torch.cat(list(map(lambda x: x[1], output)))\n",
    "labels = torch.cat(list(map(lambda x: x[0], output)))\n",
    "\n",
    "print(\"Test accuracy: {}\".format((labels == predictions).sum().float() / len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
